{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from dataHandler.preprocessor import Preprocessor\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accesing data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/michal/Desktop/UniversityOfEssex/MasterProject/22-24_CE901-CE911-CF981-SU_kaczmarczyk_michal_p/data/Bloomberg/\"\n",
    "files = os.listdir(directory)\n",
    "files = [f for f in files if os.path.isfile(directory+'/'+f)]\n",
    "files\n",
    "files = [\"RR.L.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = \"/home/michal/Desktop/UniversityOfEssex/MasterProject/22-24_CE901-CE911-CF981-SU_kaczmarczyk_michal_p/data/data/\"\n",
    "dataset = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(dataDir + file).set_index(\"Date\")\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    bl = pd.read_csv(directory + file).set_index(\"Date\")\n",
    "    bl.index = pd.to_datetime(bl.index)\n",
    "    bl = bl[\"Price Earnings Ratio \\n(P/E) \\n(RR/ LN Equity)\"]\n",
    "    bl = bl.resample('D')\n",
    "    bl = bl.ffill()\n",
    "    bl = bl.bfill()\n",
    "    df = df.join(bl, how=\"inner\")\n",
    "    dataset.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Splitting data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = []\n",
    "testDataset = []\n",
    "preprocessor = Preprocessor()\n",
    "i=0\n",
    "for df in dataset:\n",
    "    df.insert(0, \"Date\", df.index)\n",
    "    df = preprocessor.leaveDataSinceDate(df)\n",
    "    #if files[i] == \"RR.L.csv\":\n",
    "    train, test = model_selection.train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    trainDataset.append(train)\n",
    "    testDataset.append(test)\n",
    "    #else:\n",
    "    #    trainDataset.append(df)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocessing data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "for i in range(len(trainDataset)):\n",
    "    trainDataset[i] = preprocessor.dropUnnamedColumn(trainDataset[i])\n",
    "    trainDataset[i] = preprocessor.dropDuplicates(trainDataset[i])\n",
    "    trainDataset[i] = preprocessor.addChangeFeature(trainDataset[i])\n",
    "    columns = trainDataset[i].columns\n",
    "    trainDataset[i][columns[1:]] = preprocessor.fillMissingData(trainDataset[i][columns[1:]])\n",
    "    trainDataset[i][columns[1:]] = preprocessor.scaleData(trainDataset[i][columns[1:]])\n",
    "    trainDataset[i] = preprocessor.sortValuesByDate(trainDataset[i])\n",
    "    trainDataset[i].set_index(\"Date\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "for i in range(len(testDataset)):\n",
    "    testDataset[i] = preprocessor.dropUnnamedColumn(testDataset[i])\n",
    "    testDataset[i] = preprocessor.dropDuplicates(testDataset[i])\n",
    "    testDataset[i] = preprocessor.addChangeFeature(testDataset[i])\n",
    "    columns = testDataset[i].columns\n",
    "    testDataset[i][columns[1:]] = preprocessor.fillMissingData(testDataset[i][columns[1:]])\n",
    "    testDataset[i][columns[1:]] = preprocessor.scaleData(testDataset[i][columns[1:]])\n",
    "    testDataset[i] = preprocessor.sortValuesByDate(testDataset[i])\n",
    "    testDataset[i].set_index(\"Date\", inplace = True)\n",
    "\n",
    "preprocessor.saveScalers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Price Earnings Ratio \\n(P/E) \\n(RR/ LN Equity)</th>\n",
       "      <th>Change[%]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-06-27</th>\n",
       "      <td>0.959673</td>\n",
       "      <td>0.969492</td>\n",
       "      <td>0.968438</td>\n",
       "      <td>0.974500</td>\n",
       "      <td>0.902035</td>\n",
       "      <td>0.035655</td>\n",
       "      <td>0.081934</td>\n",
       "      <td>0.308960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-30</th>\n",
       "      <td>0.978050</td>\n",
       "      <td>0.970217</td>\n",
       "      <td>0.968438</td>\n",
       "      <td>0.974500</td>\n",
       "      <td>0.902035</td>\n",
       "      <td>0.044695</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.308960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-01</th>\n",
       "      <td>0.973966</td>\n",
       "      <td>0.969581</td>\n",
       "      <td>0.975565</td>\n",
       "      <td>0.975520</td>\n",
       "      <td>0.902990</td>\n",
       "      <td>0.034988</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.310437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-02</th>\n",
       "      <td>0.981113</td>\n",
       "      <td>0.976275</td>\n",
       "      <td>0.961311</td>\n",
       "      <td>0.953080</td>\n",
       "      <td>0.881989</td>\n",
       "      <td>0.034193</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.276492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-03</th>\n",
       "      <td>0.954569</td>\n",
       "      <td>0.965169</td>\n",
       "      <td>0.962533</td>\n",
       "      <td>0.968380</td>\n",
       "      <td>0.896308</td>\n",
       "      <td>0.041117</td>\n",
       "      <td>0.042722</td>\n",
       "      <td>0.331562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-23</th>\n",
       "      <td>0.135479</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0.140768</td>\n",
       "      <td>0.131244</td>\n",
       "      <td>0.134145</td>\n",
       "      <td>0.086532</td>\n",
       "      <td>0.335248</td>\n",
       "      <td>0.305738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-24</th>\n",
       "      <td>0.128190</td>\n",
       "      <td>0.127087</td>\n",
       "      <td>0.134775</td>\n",
       "      <td>0.122149</td>\n",
       "      <td>0.124849</td>\n",
       "      <td>0.153849</td>\n",
       "      <td>0.335248</td>\n",
       "      <td>0.250840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-25</th>\n",
       "      <td>0.124590</td>\n",
       "      <td>0.130620</td>\n",
       "      <td>0.135250</td>\n",
       "      <td>0.132374</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.099908</td>\n",
       "      <td>0.335248</td>\n",
       "      <td>0.376795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-26</th>\n",
       "      <td>0.133218</td>\n",
       "      <td>0.139358</td>\n",
       "      <td>0.144417</td>\n",
       "      <td>0.141053</td>\n",
       "      <td>0.144171</td>\n",
       "      <td>0.091973</td>\n",
       "      <td>0.335248</td>\n",
       "      <td>0.364169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-27</th>\n",
       "      <td>0.139554</td>\n",
       "      <td>0.142329</td>\n",
       "      <td>0.150120</td>\n",
       "      <td>0.142450</td>\n",
       "      <td>0.145599</td>\n",
       "      <td>0.071070</td>\n",
       "      <td>0.350255</td>\n",
       "      <td>0.317546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2003 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close  Adj Close    Volume  \\\n",
       "Date                                                                      \n",
       "2014-06-27  0.959673  0.969492  0.968438  0.974500   0.902035  0.035655   \n",
       "2014-06-30  0.978050  0.970217  0.968438  0.974500   0.902035  0.044695   \n",
       "2014-07-01  0.973966  0.969581  0.975565  0.975520   0.902990  0.034988   \n",
       "2014-07-02  0.981113  0.976275  0.961311  0.953080   0.881989  0.034193   \n",
       "2014-07-03  0.954569  0.965169  0.962533  0.968380   0.896308  0.041117   \n",
       "...              ...       ...       ...       ...        ...       ...   \n",
       "2022-05-23  0.135479  0.134483  0.140768  0.131244   0.134145  0.086532   \n",
       "2022-05-24  0.128190  0.127087  0.134775  0.122149   0.124849  0.153849   \n",
       "2022-05-25  0.124590  0.130620  0.135250  0.132374   0.135300  0.099908   \n",
       "2022-05-26  0.133218  0.139358  0.144417  0.141053   0.144171  0.091973   \n",
       "2022-05-27  0.139554  0.142329  0.150120  0.142450   0.145599  0.071070   \n",
       "\n",
       "            Price Earnings Ratio \\n(P/E) \\n(RR/ LN Equity)  Change[%]  \n",
       "Date                                                                   \n",
       "2014-06-27                                        0.081934   0.308960  \n",
       "2014-06-30                                        0.042722   0.308960  \n",
       "2014-07-01                                        0.042722   0.310437  \n",
       "2014-07-02                                        0.042722   0.276492  \n",
       "2014-07-03                                        0.042722   0.331562  \n",
       "...                                                    ...        ...  \n",
       "2022-05-23                                        0.335248   0.305738  \n",
       "2022-05-24                                        0.335248   0.250840  \n",
       "2022-05-25                                        0.335248   0.376795  \n",
       "2022-05-26                                        0.335248   0.364169  \n",
       "2022-05-27                                        0.350255   0.317546  \n",
       "\n",
       "[2003 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating training sequences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSequence(dataX, dataY, lengthX, predictionHorizont):\n",
    "    xData = []\n",
    "    yData = []\n",
    "    for i in range(len(dataX) - lengthX - predictionHorizont):\n",
    "        xData.append(dataX[i:i+lengthX].values)\n",
    "        yData.append(dataY[(lengthX + i):(lengthX + predictionHorizont + i)].values)\n",
    "    return xData, yData\n",
    "\n",
    "features = len(trainDataset[0].iloc[0])\n",
    "samplesForPrediction = 15\n",
    "samplesPredicted = 1\n",
    "\n",
    "trainDates = trainDataset[0].index[samplesForPrediction+1:]\n",
    "testDates = testDataset[0].index[samplesForPrediction+1:]\n",
    "xTrain = []\n",
    "yTrain = []\n",
    "xTest = []\n",
    "yTest = []\n",
    "for df in trainDataset:\n",
    "    trainX, trainY = createSequence(df, df[\"Close\"], samplesForPrediction, samplesPredicted)\n",
    "    xTrain += trainX\n",
    "    yTrain += trainY\n",
    "for df in testDataset:\n",
    "    testX, testY = createSequence(df, df[\"Close\"], samplesForPrediction, samplesPredicted)\n",
    "    xTest += testX\n",
    "    yTest += testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Making pyTorch Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23066/1264527518.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  xTrain = torch.Tensor(xTrain).to(device)\n"
     ]
    }
   ],
   "source": [
    "xTrain = torch.Tensor(xTrain).to(device)\n",
    "yTrain = torch.Tensor(yTrain).to(device)\n",
    "xTest = torch.Tensor(xTest).to(device)\n",
    "yTest = torch.Tensor(yTest).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9597, 0.9695, 0.9684,  ..., 0.0357, 0.0819, 0.3090],\n",
      "         [0.9781, 0.9702, 0.9684,  ..., 0.0447, 0.0427, 0.3090],\n",
      "         [0.9740, 0.9696, 0.9756,  ..., 0.0350, 0.0427, 0.3104],\n",
      "         ...,\n",
      "         [0.9515, 0.9497, 0.9521,  ..., 0.0354, 0.0414, 0.2954],\n",
      "         [0.9484, 0.9642, 0.9562,  ..., 0.0355, 0.0414, 0.3393],\n",
      "         [0.9607, 0.9581, 0.9532,  ..., 0.0255, 0.0414, 0.2777]],\n",
      "\n",
      "        [[0.9781, 0.9702, 0.9684,  ..., 0.0447, 0.0427, 0.3090],\n",
      "         [0.9740, 0.9696, 0.9756,  ..., 0.0350, 0.0427, 0.3104],\n",
      "         [0.9811, 0.9763, 0.9613,  ..., 0.0342, 0.0427, 0.2765],\n",
      "         ...,\n",
      "         [0.9484, 0.9642, 0.9562,  ..., 0.0355, 0.0414, 0.3393],\n",
      "         [0.9607, 0.9581, 0.9532,  ..., 0.0255, 0.0414, 0.2777],\n",
      "         [0.9372, 0.9298, 0.9359,  ..., 0.0261, 0.0411, 0.2968]],\n",
      "\n",
      "        [[0.9740, 0.9696, 0.9756,  ..., 0.0350, 0.0427, 0.3104],\n",
      "         [0.9811, 0.9763, 0.9613,  ..., 0.0342, 0.0427, 0.2765],\n",
      "         [0.9546, 0.9652, 0.9625,  ..., 0.0411, 0.0427, 0.3316],\n",
      "         ...,\n",
      "         [0.9607, 0.9581, 0.9532,  ..., 0.0255, 0.0414, 0.2777],\n",
      "         [0.9372, 0.9298, 0.9359,  ..., 0.0261, 0.0411, 0.2968],\n",
      "         [0.9352, 0.9332, 0.9457,  ..., 0.0152, 0.0411, 0.3090]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1305, 0.1338, 0.1413,  ..., 0.1475, 0.3352, 0.3262],\n",
      "         [0.1377, 0.1362, 0.1443,  ..., 0.1189, 0.3352, 0.2906],\n",
      "         [0.1372, 0.1386, 0.1397,  ..., 0.0943, 0.3352, 0.2809],\n",
      "         ...,\n",
      "         [0.1312, 0.1310, 0.1359,  ..., 0.1406, 0.3375, 0.2764],\n",
      "         [0.1316, 0.1353, 0.1418,  ..., 0.1500, 0.3352, 0.3143],\n",
      "         [0.1355, 0.1345, 0.1408,  ..., 0.0865, 0.3352, 0.3057]],\n",
      "\n",
      "        [[0.1377, 0.1362, 0.1443,  ..., 0.1189, 0.3352, 0.2906],\n",
      "         [0.1372, 0.1386, 0.1397,  ..., 0.0943, 0.3352, 0.2809],\n",
      "         [0.1261, 0.1261, 0.1301,  ..., 0.1763, 0.3227, 0.2813],\n",
      "         ...,\n",
      "         [0.1316, 0.1353, 0.1418,  ..., 0.1500, 0.3352, 0.3143],\n",
      "         [0.1355, 0.1345, 0.1408,  ..., 0.0865, 0.3352, 0.3057],\n",
      "         [0.1282, 0.1271, 0.1348,  ..., 0.1538, 0.3352, 0.2508]],\n",
      "\n",
      "        [[0.1372, 0.1386, 0.1397,  ..., 0.0943, 0.3352, 0.2809],\n",
      "         [0.1261, 0.1261, 0.1301,  ..., 0.1763, 0.3227, 0.2813],\n",
      "         [0.1222, 0.1224, 0.1303,  ..., 0.1218, 0.3227, 0.2734],\n",
      "         ...,\n",
      "         [0.1355, 0.1345, 0.1408,  ..., 0.0865, 0.3352, 0.3057],\n",
      "         [0.1282, 0.1271, 0.1348,  ..., 0.1538, 0.3352, 0.2508],\n",
      "         [0.1246, 0.1306, 0.1352,  ..., 0.0999, 0.3352, 0.3768]]])\n"
     ]
    }
   ],
   "source": [
    "print(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (encEmbedding): Linear(in_features=8, out_features=128, bias=True)\n",
      "  (decEmbedding): Linear(in_features=8, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_length: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.pow(10000, torch.arange(0, d_model, 2).float() / d_model)\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, feature_size, d_model, nhead, num_layers = 6, dim_feedforward = 512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        #self.encEmbedding = nn.Embedding(feature_size, d_model)\n",
    "        #self.decEmbedding = nn.Embedding(feature_size, d_model)\n",
    "        self.encEmbedding = nn.Linear(feature_size, d_model)\n",
    "        self.decEmbedding = nn.Linear(feature_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, feature_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "      src = self.encEmbedding(src)\n",
    "      tgt = self.decEmbedding(tgt)\n",
    "      src = self.pos_encoder(src)\n",
    "      tgt = self.pos_encoder(tgt)\n",
    "\n",
    "      tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "      memory = self.transformer_encoder(src, mask = None)\n",
    "      output = self.transformer_decoder(tgt = tgt, memory = memory, tgt_mask = tgt_mask)\n",
    "      output = self.linear(output)\n",
    "      return output\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "feature_size = features  # As we're working with 1D time series\n",
    "d_model = 128  # Embedding dimension\n",
    "nhead = 8  # Number of attention heads\n",
    "num_layers = 6  # Number of transformer layers\n",
    "dim_feedforward = 512  # Dimension of the feedforward network\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerModel(feature_size, d_model, nhead, num_layers, dim_feedforward).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1987, 15, 8])\n",
      "torch.Size([1, 15, 8])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(xTrain.shape)\n",
    "loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(xTrain, yTrain), shuffle=False, batch_size=1)\n",
    "for inputData, desired in loader:\n",
    "    print(inputData.shape)\n",
    "    print(desired.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 15, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/michal/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1987, 1])) that is different to the input size (torch.Size([15, 1987, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/michal/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([485, 1])) that is different to the input size (torch.Size([15, 485, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: [0.25848904252052307], Test Loss: [0.12087209522724152]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loss, test_loss\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#model = torch.load('model.pth')\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m train_loss, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainLoop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#torch.save(model, 'model.pth')\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#550Epochs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36mtrainLoop\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, desired)\n\u001b[1;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 13\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adam.py:379\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m         (param\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_cuda) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mis_xla \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_xla)\n\u001b[1;32m    376\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=True, params and state_steps must be CUDA or XLA tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    382\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def trainLoop(epochs):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputData, desired in loader:\n",
    "            inputData = inputData.to(device)\n",
    "            desired = desired.to(device)\n",
    "            pred = model(inputData, inputData)\n",
    "            loss = loss_fn(pred, desired)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(xTrain.permute(1, 0, 2).to(device), xTrain.permute(1, 0, 2).to(device))\n",
    "            train_loss.append(loss_fn(y_pred, yTrain).cpu().numpy().tolist())\n",
    "            y_pred = model(xTest.permute(1, 0, 2).to(device), xTest.permute(1, 0, 2).to(device))\n",
    "            test_loss.append(loss_fn(y_pred, yTest).cpu().numpy().tolist())\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss[-1:]}, Test Loss: {test_loss[-1:]}')\n",
    "    return train_loss, test_loss\n",
    "\n",
    "\n",
    "#model = torch.load('model.pth')\n",
    "train_loss, test_loss = trainLoop(100)\n",
    "#torch.save(model, 'model.pth')\n",
    "\n",
    "#550Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss)\n",
    "ax.plot(test_loss)\n",
    "ax.legend([\"train loss\", \"test loss\"])\n",
    "ax.grid()\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Loss functions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Testing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model(xTrain.permute(1, 0, 2)).cpu()\n",
    "\n",
    "y_pred = y_pred.detach().numpy()\n",
    "\n",
    "\n",
    "# Calculate the mean squared error across all outputs\n",
    "mse_total = mean_squared_error(yTrain.cpu().detach().numpy(), y_pred)\n",
    "print(f'Total Mean Squared Error: {mse_total}')\n",
    "\n",
    "set = 5*2\n",
    "real = np.concatenate(yTrain.cpu().detach().numpy()[::samplesPredicted])\n",
    "predictions = np.concatenate(y_pred[::samplesPredicted])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#ax.plot(trainDates, real, zorder = 3)\n",
    "#ax.plot(trainDates, predictions, color = 'black', zorder = 2)\n",
    "ax.plot(real, zorder = 3)\n",
    "ax.plot(predictions, color = 'black', zorder = 2)\n",
    "\n",
    "# Rotate date labels for better readability\n",
    "#plt.xticks(rotation=45)\n",
    "\n",
    "# Optionally, set the locator to have fewer ticks\n",
    "#locator = plt.matplotlib.dates.AutoDateLocator(minticks=10, maxticks=20)\n",
    "#ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "\n",
    "ax.legend([\"Real\", \"Prediction\"])\n",
    "#ax.axvspan(0, 4, facecolor='lightblue', alpha=0.5, zorder = 1)  # First part\n",
    "#ax.axvspan(4, 9, facecolor='lightgreen', alpha=0.5, zorder = 1)  # Second part\n",
    "#ax.axvspan(9, 14, facecolor='tomato', alpha=0.5, zorder = 1)  # Second third\n",
    "#ax.set_ylim([0, 1])\n",
    "ax.grid()\n",
    "plt.ylabel(\"Scaled closing\")\n",
    "plt.xlabel(\"Day number\")\n",
    "plt.title(\"Prediction for 1 day using last 10 days - train set\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model(xTest.permute(1, 0, 2)).cpu()\n",
    "\n",
    "y_pred = y_pred.detach().numpy()\n",
    "\n",
    "# Calculate the mean squared error across all outputs\n",
    "mse_total = mean_squared_error(yTest.cpu().detach().numpy(), y_pred)\n",
    "print(f'Total Mean Squared Error: {mse_total}')\n",
    "\n",
    "set = 5*2\n",
    "real = np.concatenate(yTest.cpu().detach().numpy()[::samplesPredicted])\n",
    "predictions = np.concatenate(y_pred[::samplesPredicted])\n",
    "\n",
    "#Rescaling\n",
    "dummyDataset = testDataset[0].tail(len(predictions))\n",
    "dummyDataset[\"Close\"] = predictions\n",
    "dummyDataset = preprocessor.scaler.inverse_transform(dummyDataset)\n",
    "predictions = dummyDataset[:,3]\n",
    "\n",
    "dummyDataset = testDataset[0].tail(len(real))\n",
    "dummyDataset[\"Close\"] = real\n",
    "dummyDataset = preprocessor.scaler.inverse_transform(dummyDataset)\n",
    "real = dummyDataset[:,3]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(testDates)[-50:], real[-50:], zorder = 3)\n",
    "ax.plot(np.array(testDates)[-50:], predictions[-50:], color = 'black', zorder = 2)\n",
    "\n",
    "# Rotate date labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Optionally, set the locator to have fewer ticks\n",
    "locator = plt.matplotlib.dates.AutoDateLocator(minticks=10, maxticks=20)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "ax.legend([\"Real\", \"Prediction\"])\n",
    "#ax.axvspan(0, 4, facecolor='lightblue', alpha=0.5, zorder = 1)  # First part\n",
    "#ax.axvspan(4, 9, facecolor='lightgreen', alpha=0.5, zorder = 1)  # Second part\n",
    "#ax.axvspan(9, 14, facecolor='tomato', alpha=0.5, zorder = 1)  # Second third\n",
    "#ax.set_ylim([0, 1])\n",
    "ax.grid()\n",
    "plt.ylabel(\"Scaled closing\")\n",
    "plt.xlabel(\"Day number\")\n",
    "plt.title(\"Prediction for 1 day using last 10 days - test set\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Profit Calculation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit = 0\n",
    "lastBuy = 0\n",
    "action = []\n",
    "for realVal, predVal in zip(real[:-samplesPredicted], predictions[samplesPredicted::samplesPredicted]):\n",
    "    if(realVal<predVal and lastBuy == 0):\n",
    "        lastBuy = realVal\n",
    "        action.append(1)\n",
    "    elif(realVal > predVal and lastBuy > 0):\n",
    "        profit += realVal - lastBuy\n",
    "        lastBuy = 0\n",
    "        action.append(-1)\n",
    "    else:\n",
    "        action.append(0)\n",
    "\n",
    "if(lastBuy>0):\n",
    "    profit += realVal - lastBuy\n",
    "    lastBuy = 0\n",
    "    action.append(-1)\n",
    "else:\n",
    "    action.append(0)\n",
    "\n",
    "print(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = real[:]\n",
    "predictions = predictions[:]\n",
    "testDates = testDates[:]\n",
    "action = np.array(action[:])\n",
    "\n",
    "buy_dates = np.array(testDates)[action == 1]\n",
    "sell_dates = np.array(testDates)[action == -1]\n",
    "\n",
    "buy_prices = real[action == 1]\n",
    "sell_prices = real[action == -1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(testDates), real, zorder = 3)\n",
    "ax.plot(np.array(testDates), predictions, color = 'black', zorder = 2)\n",
    "\n",
    "ax.scatter(buy_dates, buy_prices, color='green', marker='^', s=100, label='Buy')\n",
    "ax.scatter(sell_dates, sell_prices, color='red', marker='v', s=100, label='Sell')\n",
    "\n",
    "# Rotate date labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Optionally, set the locator to have fewer ticks\n",
    "locator = plt.matplotlib.dates.AutoDateLocator(minticks=10, maxticks=20)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "ax.legend([\"Real\", \"Prediction\"])\n",
    "ax.grid()\n",
    "plt.ylabel(\"Scaled closing\")\n",
    "plt.xlabel(\"Day number\")\n",
    "plt.title(\"Prediction for 1 day using last 10 days - test set\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
